{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be5b40d-dfde-44d5-bda5-265666590f8e",
   "metadata": {},
   "source": [
    "### DTIHAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8be840c7-da82-4644-8c45-c890525a8568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import dgl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.io as sio\n",
    "\n",
    "from random import sample\n",
    "from scipy import linalg\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import KFold  # k-folds CV\n",
    "\n",
    "from utils import EarlyStopping\n",
    "from model_hetero import HAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "235088f7-0406-4fbd-998e-fe8ab884f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HAN embedding 函数\n",
    "\n",
    "def DTI_embedding(param):\n",
    "    # param 参数字典\n",
    "\n",
    "    # 解析参数\n",
    "    dgc = param.get('dgc')\n",
    "    dc = param.get('dc')\n",
    "    drugAttribute = param.get('drugAttribute')\n",
    "    dg = param.get('dg')\n",
    "    targetAttribute = param.get('targetAttribute')\n",
    "\n",
    "    drug_max_iters = param.get('drug_max_iters')\n",
    "    target_max_iters = param.get('target_max_iters')\n",
    "    \n",
    "    drug_hidden_size = param.get('drug_hidden_size')\n",
    "    drug_out_size = param.get('drug_out_size')\n",
    "    drug_num_heads = param.get('drug_num_heads')\n",
    "    drug_dropout = param.get('drug_dropout')\n",
    "    drug_label = param.get('drug_label')\n",
    "\n",
    "    target_hidden_size = param.get('target_hidden_size')\n",
    "    target_out_size = param.get('target_out_size')\n",
    "    target_num_heads = param.get('target_num_heads')\n",
    "    target_dropout = param.get('target_dropout')\n",
    "    target_label = param.get('target_label')\n",
    "\n",
    "    path_to_csv = param.get('path_to_csv')\n",
    "\n",
    "    # 源节点 -> 目标节点\n",
    "    begin_dgc,end_dgc=[],[]\n",
    "    for i in range(np.shape(dgc)[0]):\n",
    "        for j in range(np.shape(dgc)[1]):\n",
    "            if dgc[i][j]==1:\n",
    "                begin_dgc.append(j)\n",
    "                end_dgc.append(i)\n",
    "\n",
    "    # 药物，边\n",
    "    begin_dc,end_dc,sim_dc=[],[],[]\n",
    "    drug_edge_dict = {}\n",
    "    edge, edge_index = None, None\n",
    "    for i in range(np.shape(dc)[0]):\n",
    "        for j in range(0,np.shape(dc)[1]):\n",
    "            begin_dc.append(i)\n",
    "            end_dc.append(j)\n",
    "            edge=str(i)+'=>'+str(j)\n",
    "            edge_index=len(sim_dc)\n",
    "            sim_dc.append(dc[i][j])\n",
    "            drug_edge_dict[edge] = edge_index\n",
    "\n",
    "    #创建异构图drug_target_1来获得药物隐特征，选取药物-靶标-药物和药物-药物-药物作为元路径\n",
    "    drug_target_1 = dgl.heterograph({\n",
    "         ('drug','dt','target'):(begin_dgc,end_dgc),\n",
    "         ('target','td','drug'):(end_dgc,begin_dgc),\n",
    "         ('drug','dd','drug'):(begin_dc,end_dc),\n",
    "         ('drug','dd_r','drug'):(end_dc,begin_dc),\n",
    "    })\n",
    "    sim_dc=np.array(sim_dc)\n",
    "    #加入药物间的相似度特征\n",
    "    drug_target_1.edges['dd'].data['sim']=t.from_numpy(sim_dc)\n",
    "    #输入药物的初始节点特征\n",
    "    drug_target_1.nodes['drug'].data['feature']=t.from_numpy(drugAttribute).float()\n",
    "\n",
    "    # 靶标，边\n",
    "    begin_dg,end_dg,sim_dg = [],[],[]\n",
    "    target_edge_dict = {}\n",
    "    edge, edge_index = None, None\n",
    "    for i in range(np.shape(dg)[0]):\n",
    "        for j in range(0,np.shape(dg)[1]):\n",
    "            begin_dg.append(i)\n",
    "            end_dg.append(j)\n",
    "            edge=str(i)+'=>'+str(j)\n",
    "            edge_index=len(sim_dg)\n",
    "            sim_dg.append(dg[i][j])\n",
    "            target_edge_dict[edge] = edge_index\n",
    "\n",
    "    #创建异构图drug_target_2来获得靶标隐特征，选取靶标-药物-靶标和靶标-靶标-靶标作为元路径\n",
    "    drug_target_2 = dgl.heterograph({\n",
    "         ('target','td','drug'):(end_dgc,begin_dgc),\n",
    "         ('drug','dt','target'):(begin_dgc,end_dgc),\n",
    "         ('target','tt','target'):(begin_dg,end_dg),\n",
    "         ('target','tt_r','target'):(end_dg,begin_dg),\n",
    "    })\n",
    "    sim_dg=np.array(sim_dg)\n",
    "    #加入靶标间的相似度特征\n",
    "    drug_target_2.edges['tt'].data['sim']=t.from_numpy(sim_dg)\n",
    "    #输入靶标的初始节点特征\n",
    "    drug_target_2.nodes['target'].data['feature']=t.from_numpy(targetAttribute).float()\n",
    "\n",
    "    # 药物网络训练\n",
    "    drug_features = drug_target_1.nodes['drug'].data['feature'].to('cuda:0')\n",
    "    drug_model = HAN(meta_paths=[['dt','td'],['dd','dd_r']],\n",
    "                     in_size=drug_features.shape[1],\n",
    "                     hidden_size=drug_hidden_size,\n",
    "                     out_size=drug_out_size,\n",
    "                     num_heads=drug_num_heads,\n",
    "                     dropout=drug_dropout).to('cuda:0')\n",
    "    drug_target_1 = drug_target_1.to('cuda:0')\n",
    "    # stopper = EarlyStopping(patience=100)\n",
    "    loss_fcn = t.nn.MSELoss()\n",
    "    optimizer = t.optim.Adam(drug_model.parameters(), lr=0.006,\n",
    "                                     weight_decay=0.001)\n",
    "    for epoch in range(drug_max_iters):\n",
    "        drug_model.train()\n",
    "        logits = drug_model(drug_target_1, drug_features)\n",
    "        loss = loss_fcn(drug_target_1.edges['dd'].data['sim'].float(), (logits@logits.T).view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 50 == 0:\n",
    "            print('Epoch {:d} | Train Loss {:.4f}'.format(epoch+1,loss.item()))\n",
    "    \n",
    "    logits = logits.cpu().detach().numpy()\n",
    "    # 保存数据\n",
    "    drug_result = pd.DataFrame(logits)\n",
    "    drug_result.index = drug_label\n",
    "\n",
    "    #获取靶标隐特征\n",
    "    #特征维度=hidden_size*num_heads\n",
    "    print('target embedding...')\n",
    "    target_features = drug_target_2.nodes['target'].data['feature'].to('cuda:0')\n",
    "    target_model = HAN(meta_paths=[['td','dt'],['tt','tt_r']],\n",
    "                       in_size=target_features.shape[1],\n",
    "                       hidden_size=target_hidden_size,\n",
    "                       out_size=target_out_size,\n",
    "                       num_heads=target_num_heads,\n",
    "                       dropout=target_dropout).to('cuda:0')\n",
    "    drug_target_2 = drug_target_2.to('cuda:0')\n",
    "    # stopper = EarlyStopping(patience=100)\n",
    "    loss_fcn = t.nn.MSELoss()\n",
    "    optimizer = t.optim.Adam(target_model.parameters(), lr=0.006,\n",
    "                                     weight_decay=0.001)\n",
    "    \n",
    "    for epoch in range(target_max_iters):\n",
    "        target_model.train()\n",
    "        logits = target_model(drug_target_2, target_features)\n",
    "        loss = loss_fcn(drug_target_2.edges['tt'].data['sim'].float(), (logits@logits.T).view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 50 == 0:\n",
    "            print('Epoch {:d} | Train Loss {:.4f}'.format(epoch+1,loss.item()))\n",
    "        \n",
    "    logits = logits.cpu().detach().numpy()\n",
    "    # 保存数据\n",
    "    target_result = pd.DataFrame(logits)\n",
    "    target_result.index = target_label\n",
    "\n",
    "    # 输出保存\n",
    "    df = pd.concat([target_result, drug_result], axis=0)\n",
    "    # out_result = target_result.append(drug_result)\n",
    "    df.to_csv(path_to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de1cefd4-d613-4f50-af99-61229b8201c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评价函数\n",
    "# 计算 ROC 的 AUC 值\n",
    "def AUCValue(label, score):\n",
    "    label = np.copy(label)\n",
    "    score = np.copy(score)\n",
    "    fpr, tpr, thresholds = roc_curve(label, score, pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return roc_auc\n",
    "\n",
    "# 计算 PR 的 AUC 值\n",
    "def PRAUCValue(label, score):\n",
    "    label = np.copy(label)\n",
    "    score = np.copy(score)\n",
    "    pr_auc = average_precision_score(label, score)\n",
    "    return pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5e89c6a-0f28-40cd-97d8-17b7da4411f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPRM class\n",
    "class FPRM:\n",
    "    '''\n",
    "    类属性：\n",
    "    特征投影矩阵 R\n",
    "    正则化参数 re_lambda\n",
    "\n",
    "    类成员：（尽量与 sklearn 一致）\n",
    "    训练函数 fit\n",
    "    预测函数 predict_pair\n",
    "    预测函数 predict_matrix\n",
    "    '''\n",
    "\n",
    "    # 初始化函数\n",
    "    def __init__(self,\n",
    "                 re_lambda = None):\n",
    "        self.R = None\n",
    "        self.re_lambda = re_lambda\n",
    "\n",
    "    # 计算矩阵逆，如果矩阵不可逆，求其最小二乘逆\n",
    "    def inverse_matrix(self,X):\n",
    "        X_inv = None\n",
    "        try:\n",
    "            X_inv = linalg.inv(X)\n",
    "        except:\n",
    "            print('sigular matrix, compute linalg.lstsq solve inv(X)')\n",
    "            n,m = X.shape\n",
    "            Ematrix = np.eye(n,n,dtype=float)\n",
    "            X_inv = linalg.lstsq(X,Ematrix)[0]\n",
    "        finally:\n",
    "            return X_inv\n",
    "\n",
    "    # 训练函数\n",
    "    def fit(self,X,Y,Delta):\n",
    "        '''\n",
    "\n",
    "        :param X:  特征矩阵，一行一个样本, N * m, N 个样本，m 个特征\n",
    "        :param Y:  特征矩阵，一行一个样本， N * n, N 个样本，n 个特征\n",
    "        :param Delta: 对应着特征矩阵 X,Y 的关系作用矩阵，维度 N * N，x_i R y_j^T = delta_{ij}\n",
    "        :return: None\n",
    "        '''\n",
    "\n",
    "        x = np.copy(X)\n",
    "        y = np.copy(Y)\n",
    "        delta = np.copy(Delta)\n",
    "\n",
    "        xTx = x.T.dot(x)   # x^T * x\n",
    "        yTy = y.T.dot(y)   # y^T * y\n",
    "        xTDy = x.T.dot(delta).dot(y)\n",
    "        if self.re_lambda is None:\n",
    "            # 没有正则项\n",
    "            xTx_inv = self.inverse_matrix(xTx)\n",
    "            yTy_inv = self.inverse_matrix(yTy)\n",
    "            self.R = xTx_inv.dot(xTDy).dot(yTy_inv)\n",
    "        else:\n",
    "            # 没有正则项，需要求解 Sylvester 方程\n",
    "            yTy_inv = self.inverse_matrix(yTy)\n",
    "            B = self.re_lambda * yTy_inv\n",
    "            Q = xTDy.dot(yTy_inv)\n",
    "            # 注意，scipy.linalg.solve_sylvester 在 scipy 1.1.0 版本之后\n",
    "            self.R = linalg.solve_sylvester(a=xTx,b=B,q=Q)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    # 预测函数，矩阵形式\n",
    "    def predict_matrix(self,X,Y):\n",
    "        '''\n",
    "\n",
    "        :param X: 特征矩阵，一行一个样本\n",
    "        :param Y: 特征矩阵，一行一个样本， X,Y 样本个数必须一致，特征维度可以不一致\n",
    "        :return: 预测关系矩阵\n",
    "        '''\n",
    "        x = np.copy(X)\n",
    "        y = np.copy(Y)\n",
    "\n",
    "        return x.dot(self.R).dot(y.T)\n",
    "\n",
    "    # 预测函数，pair 形式\n",
    "    def predict_pair(self,X,Y):\n",
    "        '''\n",
    "\n",
    "        :param X: 特征矩阵，一行一个样本\n",
    "        :param Y: 特征矩阵，一行一个样本， X,Y 样本个数必须一致，特征维度可以不一致\n",
    "        :return: 返回对应样本数的向量\n",
    "        '''\n",
    "        n,m = X.shape\n",
    "        val = np.zeros((n,1),dtype=float)\n",
    "        for i in range(n):\n",
    "            val[i] = X[i].dot(self.R).dot(Y[i].T)\n",
    "\n",
    "        return val\n",
    "\n",
    "\n",
    "# 特征投影关系隶属度类，继承 FPRM 类\n",
    "# 会新添一些属性：\n",
    "# ulabel  唯一标签 list\n",
    "# dict_label  唯一标签，dict\n",
    "# mu,sigma  隶属度函数参数，每一个标签对应一组参数\n",
    "class FPRMmembership(FPRM):\n",
    "    # 初始化函数\n",
    "    def __init__(self,\n",
    "                 re_lambda=None):\n",
    "        FPRM.__init__(self, re_lambda=re_lambda)\n",
    "        # 下面是新增属性\n",
    "        self.ulabel = None     # 唯一标签，list\n",
    "        self.dict_label = None    # 唯一标签，dict\n",
    "        self.mu = None         # 对应类别标签的隶属度函数均值参数, dict 保存，易于参数值与标签对应\n",
    "        self.sigma2 = None     # 对应类别标签的隶属度函数方差参数，dict 保存，易于参数值域标签对应\n",
    "\n",
    "    # 训练函数\n",
    "    def fit(self,X,Y,Delta):\n",
    "        '''\n",
    "\n",
    "        :param X:  特征矩阵，一行一个样本, N * m, N 个样本，m 个特征\n",
    "        :param Y:  特征矩阵，一行一个样本， N * n, N 个样本，n 个特征\n",
    "        :param Delta: 对应着特征矩阵 X,Y 的关系作用矩阵，维度 N * N，x_i R y_j^T = delta_{ij}\n",
    "        :return: None\n",
    "        '''\n",
    "\n",
    "        delta = np.array(Delta)\n",
    "        # 矩阵中不重复元素\n",
    "        delta_list = delta.flatten()\n",
    "        ulabel = np.unique(delta_list).tolist()   # 转成 list 形式\n",
    "\n",
    "        # 计算 R\n",
    "        FPRM.fit(self,X=X,Y=Y,Delta=Delta)\n",
    "        # 计算训练集的预测值\n",
    "        pred_delta = FPRM.predict_matrix(self,X=X,Y=Y)\n",
    "\n",
    "        # 构造标签的索引字典，方便读取索引值\n",
    "        D = {}\n",
    "        mu = {}\n",
    "        sigma2 = {}\n",
    "        for i,val in enumerate(ulabel):\n",
    "            D[val] = i      # 标签值为 val 对应 ulabel 的索引为 i, 即 ulabel[i] = val\n",
    "            # 计算均值与方差\n",
    "            mu[val] = np.mean(pred_delta[ Delta == val])\n",
    "            sigma2[val] = np.var(pred_delta[Delta == val])\n",
    "            # print('label : %s , mean : %.4f , var : %.4f' %(val,mu[val],sigma2[val]))\n",
    "\n",
    "        self.ulabel = ulabel\n",
    "        self.dict_label = D\n",
    "        self.mu = mu\n",
    "        self.sigma2 = sigma2\n",
    "        return self\n",
    "\n",
    "    # 预测函数，矩阵形式，返回指定标签的隶属度值\n",
    "    def predict_score_matrix(self,X,Y,label):\n",
    "        '''\n",
    "\n",
    "        :param X: 特征矩阵，一行一个样本\n",
    "        :param Y: 特征矩阵，一行一个样本， X,Y 样本个数必须一致，特征维度可以不一致\n",
    "        :param label: 需要计算隶属度的标签\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        predict_value = X.dot(self.R).dot(Y.T)\n",
    "        if label in self.ulabel:\n",
    "            val2 = np.square(predict_value - self.mu[label])\n",
    "            membership = np.exp(- val2 / (2.0 * self.sigma2[label]))\n",
    "            return membership\n",
    "        else:\n",
    "            print('interaciton label error!')\n",
    "            exit()\n",
    "\n",
    "\n",
    "    #预测函数，pair 形式，返回指定标签的隶属度值\n",
    "    def predict_score_pair(self,X,Y,label):\n",
    "        '''\n",
    "\n",
    "        :param X: 特征矩阵，一行一个样本\n",
    "        :param Y: 特征矩阵，一行一个样本， X,Y 样本个数必须一致，特征维度可以不一致\n",
    "        :param label: 指定需要返回标签的隶属度\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        n = X.shape[0]\n",
    "        val = np.zeros((n,1),dtype=float)\n",
    "        for i in range(n):\n",
    "            val[i] = X[i].dot(self.R).dot(Y[i].T)\n",
    "\n",
    "        if label in self.ulabel:\n",
    "            val2 = np.square(val - self.mu[label])\n",
    "            membership = np.exp(- val2 / (2.0 * self.sigma2[label]))\n",
    "            return membership\n",
    "        else:\n",
    "            print('interaction label error!')\n",
    "            exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04bb3076-5d7f-413e-864b-2715a4bd2dc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 交叉验证\n",
    "\n",
    "def get_k_folds(tsize,kfolds=10):\n",
    "    '''\n",
    "\n",
    "    :param tsize:  需要交叉验证的样本总数\n",
    "    :param kfolds:\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    indices = np.array(range(tsize))\n",
    "    np.random.shuffle(indices)\n",
    "    testFolds = []\n",
    "    trainFolds = []\n",
    "    skf = KFold(n_splits=kfolds)\n",
    "    for trIndex, teIndex in skf.split(indices):\n",
    "        Xtr,Xte = indices[trIndex], indices[teIndex]\n",
    "        testFolds.append(Xte.tolist())\n",
    "        trainFolds.append(Xtr.tolist())\n",
    "\n",
    "    return testFolds,trainFolds\n",
    "\n",
    "# 用于计算 AUC, AUPR 的交叉验证函数，确保交叉后每一行、每一列至少一个非 0\n",
    "def doCrossVal(inMat,kfold=10,numSplit=5):\n",
    "    '''\n",
    "    :param inMat: 相互作用矩阵，其值为 {0,1}\n",
    "    :param kfold:  交叉折数\n",
    "    :param numSplit:  重复次数\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    tempMat = np.array(np.copy(inMat))\n",
    "    # deal with one position\n",
    "    # all one position\n",
    "    rowIndexs, colIndexs = np.where(tempMat == 1)\n",
    "    df_onePos = pd.DataFrame({'rowIndexs': rowIndexs, 'colIndexs': colIndexs}, columns=['rowIndexs', 'colIndexs'])\n",
    "\n",
    "    nLink = 1  # 至少需要满足 nLink+1 的作用关系\n",
    "    df_row_size = df_onePos.groupby('rowIndexs')['colIndexs'].size()  # 按 rowIndexs 分组，统计属性 colIndexs 值的个数\n",
    "    ind = df_row_size.index[df_row_size.values > nLink]\n",
    "    df_onePos = df_onePos[df_onePos['rowIndexs'].isin(ind)]\n",
    "    df_col_size = df_onePos.groupby('colIndexs')['rowIndexs'].size()  # 按 colIndexs 分组，size() 函数作用在 rowIndexs 属性上\n",
    "    ind = df_col_size.index[df_col_size.values > nLink]\n",
    "    df_onePos = df_onePos[df_onePos['colIndexs'].isin(ind)]\n",
    "\n",
    "    # 为了确保能够进行安全的交叉验证，一个好的想法是：\n",
    "    # 先把不可进行交叉的作用关系剔除（该行或列只有一个 1）（上述代码已经完成这项工作）\n",
    "    # 其次，再给每行、列都随机保留一个 1，这样剩余的 1 进行交叉验证，无论如何划分，都可以确保每行、列至少有一个 1\n",
    "    # randomly remove [one row], when 'col' > 1\n",
    "    grouped_size = df_onePos.groupby(by='colIndexs')['rowIndexs'].size()\n",
    "    dupCol = grouped_size.index[grouped_size.values > 1].values\n",
    "    if len(dupCol) > 0:\n",
    "        tmp = df_onePos[df_onePos['colIndexs'].isin(dupCol)]\n",
    "        grouped_col = tmp.groupby(by='colIndexs')\n",
    "        del_row_indexs = []\n",
    "        del_col_indexs = []\n",
    "        for name, group in grouped_col:\n",
    "            del_col_indexs.append(int(name))\n",
    "            del_row_indexs.append(sample(list(group['rowIndexs'].values), 1)[0])  # 随机的抽取一个\n",
    "        df_del_indexs = pd.DataFrame({'rowIndexs': del_row_indexs, 'colIndexs': del_col_indexs},\n",
    "                                     columns=['rowIndexs', 'colIndexs'])\n",
    "\n",
    "        # 采用求差集的方式从 df_onePos 中删除 df_del_indexs\n",
    "        # 由于 DataFrame 中没有求差集操作，先如下进行\n",
    "        # df_del_indexs 已经全部在 df_onePos 中，并且 df_onePos 中并无重复元素，因此，可以把 df_del_indexs 拼接到 df_onePos 中\n",
    "        # 然后，删除所有重复元素即可\n",
    "        # 去重，利用drop_duplicates方法，a=a.drop_duplicates()，以及设置参数keep=False，意思就是只要有重复，重复的记录都去掉\n",
    "        # df_onePos = df_onePos.append(df_del_indexs)  # 拼接\n",
    "        df_onePos = pd.concat([df_onePos, df_del_indexs])  # 拼接\n",
    "        df_onePos = df_onePos.drop_duplicates(keep=False)  # 去重，并且不保留重复记录\n",
    "\n",
    "    # randomly remove [one col] , when 'row' > 1\n",
    "    grouped_size = df_onePos.groupby(by='rowIndexs')['colIndexs'].size()\n",
    "    dupRow = grouped_size.index[grouped_size.values > 1].values\n",
    "    if len(dupRow) > 0:\n",
    "        tmp = df_onePos[df_onePos['rowIndexs'].isin(dupRow)]\n",
    "        grouped_row = tmp.groupby(by='rowIndexs')\n",
    "        del_row_indexs = []\n",
    "        del_col_indexs = []\n",
    "        for name, group in grouped_row:\n",
    "            del_row_indexs.append(int(name))\n",
    "            del_col_indexs.append(sample(list(group['colIndexs'].values), 1)[0])  # 随机抽取一列\n",
    "\n",
    "        df_del_indexs = pd.DataFrame({'rowIndexs': del_row_indexs, 'colIndexs': del_col_indexs},\n",
    "                                    columns=['rowIndexs', 'colIndexs'])\n",
    "        # 取 df_onePos - df_del_indexs 差集\n",
    "        # df_onePos = df_onePos.append(df_del_indexs)  # 拼接\n",
    "        df_onePos = pd.concat([df_onePos, df_del_indexs]) # 拼接\n",
    "        df_onePos = df_onePos.drop_duplicates(keep=False)  # 去重，并且不保留重复记录\n",
    "\n",
    "    # deal with zeros position\n",
    "    rowIndexs, colIndexs = np.where(tempMat == 0)\n",
    "    df_zerosPos = pd.DataFrame({'rowIndexs': rowIndexs, 'colIndexs': colIndexs}, columns=['rowIndexs', 'colIndexs'])\n",
    "\n",
    "    # 检查是否剩余足够多正例进行交叉验证\n",
    "    numOne = df_onePos.shape[0]  # 正例个数\n",
    "    if numOne < kfold:\n",
    "        print('available number of ONEs is less than %s' % kfold)\n",
    "        sys.exit()\n",
    "    numZero = df_zerosPos.shape[0]\n",
    "    if numZero < kfold:\n",
    "        print('available number of ZEROs is less than %s' % kfold)\n",
    "        sys.exit()\n",
    "\n",
    "    # 生成交叉验证数据集\n",
    "    savedFolds = []  # 保存交叉数据集，长度等于重复的次数，每一个元素保存一个 nfold 折交叉验证\n",
    "                     # 训练集保存将测试集对应作用关系置为 0 的作用矩阵\n",
    "                     # 测试集保存为 DataFrame, 3 列 （rowIndexs,colIndexs,value）\n",
    "    for i in range(numSplit):\n",
    "        # 第 i 次重复\n",
    "        list_i = []  # 保存第 i 个 split 的 nfold cv 数据\n",
    "        # 只要挑出测试集，训练集只需要将原始作用矩阵对应的测试集为 1 的值置为 0 即可\n",
    "        # 因此，可以不对无需对 get_k_folds 返回的训练集进行处理\n",
    "        # 虽然 ONE, ZERO 分开处理的，但是对于 ZERO 部分，无需重新置 0，只需将 ONE 部分置 0\n",
    "        # for ONEs\n",
    "        test_one_Folds, trainFolds = get_k_folds(tsize=numOne, kfolds=kfold)\n",
    "        # for ZEROs\n",
    "        test_zero_Folds, trainFolds = get_k_folds(tsize=numZero, kfolds=kfold)\n",
    "        for j in range(kfold):\n",
    "            dict_j = {}  # 以字典形式保存第 j 折的数据\n",
    "            # 测试集\n",
    "            # for ONEs\n",
    "            one_Index = test_one_Folds[j]\n",
    "            one_rowIndexs = df_onePos.iloc[one_Index]['rowIndexs'].values\n",
    "            one_colIndexs = df_onePos.iloc[one_Index]['colIndexs'].values\n",
    "            one_testSet = df_onePos.iloc[one_Index][['rowIndexs', 'colIndexs']]  # DataFrame 格式\n",
    "            one_testSet['value'] = 1  # 增加一列\n",
    "            # for ZEROs\n",
    "            # 由于 ZEROs 无需在 interaction 矩阵置 0 ，可以不用提取 row/col Index, 而是直接提取\n",
    "            # 对应的 DataFrame\n",
    "            zero_Index = test_zero_Folds[j]\n",
    "            zero_testSet = df_zerosPos.iloc[zero_Index][['rowIndexs', 'colIndexs']]  # DataFrame 格式\n",
    "            zero_testSet['value'] = 0  # 增加一列\n",
    "\n",
    "            # one_testSet 与 zero_testSet 合并起来就是最终的测试集，且 DataFrame 格式\n",
    "            # testSet = one_testSet.append(zero_testSet)\n",
    "            testSet = pd.concat([one_testSet, zero_testSet], axis=0)\n",
    "        \n",
    "\n",
    "            # 训练集\n",
    "            # 只需要在原始的 interaction 矩阵中，将 one_testSet 对应的 1 置为 0 即可\n",
    "            tmp = np.copy(tempMat)  # data 为原始的作用矩阵，np.array 格式\n",
    "            tmp[one_rowIndexs, one_colIndexs] = 0  # 置 0\n",
    "\n",
    "            # 保存\n",
    "            dict_j['testSet'] = testSet  # DataFrame   [rowIndexs,colIndexs,value]\n",
    "            dict_j['foldMat'] = tmp  # np.array\n",
    "\n",
    "            list_i.append(dict_j)\n",
    "\n",
    "        savedFolds.append(list_i)\n",
    "\n",
    "    return savedFolds\n",
    "\n",
    "# cross validation funciton to calculate AUPR and AUC\n",
    "def CV_AUPR_AUC(interactionMatrix,drugAttribute,targetAttribute,re_lambda,\n",
    "                numSplit=3,kfolds=10):\n",
    "    '''\n",
    "\n",
    "    :param interactionMatrix:  drug-target interaction matrix, target * drug\n",
    "    :param drugAttribute  drug 特征， drug 数 * 特征维数\n",
    "    :param targetAttribute target 特征，target 数 * 特征维数\n",
    "    :param re_lambda   FPRM 正则化参数\n",
    "    :param numSplit: 重复次数\n",
    "    :param kfolds:   交叉次数\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    inMat = np.copy(interactionMatrix)\n",
    "    drug_attr = np.copy(drugAttribute)\n",
    "    target_attr = np.copy(targetAttribute)\n",
    "\n",
    "    # 交叉验证\n",
    "    savedFolds = doCrossVal(inMat=inMat,kfold=kfolds,numSplit=numSplit)\n",
    "    # 在 savedFolds 中的作用关系矩阵 foldMat 与输入关系矩阵 inMat 形式上一致\n",
    "    # 由于在这里 inMat 输入时是 target * drug 矩阵，因此，foldMat 也是 target * drug 矩阵\n",
    "\n",
    "    aupr_split = []\n",
    "    auc_split = []\n",
    "    for i,spliting in enumerate(savedFolds):\n",
    "        aupr_cv = []\n",
    "        auc_cv = []\n",
    "        for j,folding in enumerate(spliting):\n",
    "            # 准备数据集， train, test\n",
    "\n",
    "            # train data\n",
    "            # 交叉验证中已经返回了将测试集置 0 的作用矩阵，直接读取即可\n",
    "            delta = folding['foldMat']       # target * drug 作用矩阵\n",
    "\n",
    "            # FPRM 模型的预测得分 score\n",
    "            clf = FPRMmembership(re_lambda=re_lambda)\n",
    "            clf.fit(X=target_attr,Y=drug_attr,Delta=delta)\n",
    "            score = clf.predict_score_matrix(X=target_attr,Y=drug_attr,label=1) # target * drug 得分矩阵\n",
    "            #print(delta.shape,target_attr.shape,drug_attr.shape,score.shape)\n",
    "            #print(delta)\n",
    "            #print(score)\n",
    "            #(26, 54) (26, 25) (54, 25) (26, 54)\n",
    "            # AUPR、AUC 值\n",
    "            # 提取出 testSet 中所有 score\n",
    "            testSet = folding['testSet']   # DataFrame  ['rowIndexs','colIndexs','value']\n",
    "            rowIndexs = testSet['rowIndexs'].values\n",
    "            colIndexs = testSet['colIndexs'].values\n",
    "            test_score = score[rowIndexs,colIndexs]\n",
    "            test_label = testSet['value'].values\n",
    "            \n",
    "            aupr = PRAUCValue(label=test_label,score=test_score)\n",
    "            aupr_cv.append(aupr)\n",
    "            auc_val = AUCValue(label=test_label, score=test_score)\n",
    "            auc_cv.append(auc_val)\n",
    "\n",
    "        aupr_split.append(np.mean(aupr_cv))\n",
    "        auc_split.append(np.mean(auc_cv))\n",
    "\n",
    "\n",
    "    aupr_mean = np.mean(np.array(aupr_split))\n",
    "    aupr_std = np.std(np.array(aupr_split))\n",
    "    auc_mean = np.mean(np.array(auc_split))\n",
    "    auc_std = np.std(np.array(auc_split))\n",
    "    return (aupr_mean,aupr_std,auc_mean,auc_std,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c056c7bf-9391-4ce1-ac4f-c287b1b5b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数\n",
    "curr_data_number = 3   # 当前数据集\n",
    "\n",
    "# 药物网络参数\n",
    "drug_hidden_sizes=[70,46,23,5]\n",
    "drug_num_heads_array=[3,2,5,5]\n",
    "drug_out_size = [16, 16, 16, 16]\n",
    "drug_dropout = 0.4\n",
    "# 靶标网络参数\n",
    "target_hidden_sizes=[70,46,23,5]\n",
    "target_num_heads_array=[3,2,5,5]\n",
    "target_out_size = [16, 16, 16, 16] # out_size 对应 model_hetero 中 HAN 输出最后一个线性层，本代码没有采用该层，所以最终 embedding维度为 hidden_size * num_heads\n",
    "target_dropout = 0.4\n",
    "# 迭代次数\n",
    "drug_max_iters = [300, 100, 200, 100]\n",
    "target_max_iters = [800, 100, 300, 200]\n",
    "\n",
    "\n",
    "# 用于 FPRM 的特征维度，与 embedding 维度保持一致\n",
    "f_n_s = [210,92,115,25]  # 特征维度\n",
    "\n",
    "# lambda_feature : 特征权重\n",
    "\n",
    "lambda_features = [0.5, 0.2, 0.4, 0.01]\n",
    "\n",
    "\n",
    "drug_f_n = f_n_s[curr_data_number]  # drug 特征维度\n",
    "target_f_n = f_n_s[curr_data_number]  # target 特征维度\n",
    "\n",
    "# 数据读取\n",
    "# 1、路径与名称\n",
    "path_embeding = 'result/'  # embedding 数据路径\n",
    "path_mds = 'data/mds-mat/'  # mds 数据路径\n",
    "path_original = 'data/original-mat/'  # 原始数据路径\n",
    "name_mds = ['Enzyme', 'GPCR', 'IonChannel', 'NuclearRecept']   # mds 数据\n",
    "name_embedding = ['e_result', 'gpcr_result', 'ic_result', 'nr_result']  # embedding 数据\n",
    "name_interaction = ['AdmatEnzyme','AdmatGPCR','AdmatIonChannel','AdmatNuclearRecept']  # 原始数据集中相互作用矩阵\n",
    "name_csim = ['CsimmatEnzyme','CsimmatGPCR','CsimmatIonChannel','CsimmatNuclearRecept']  # 原始数据集中药物-药物相似度\n",
    "name_tsim = ['TsimmatEnzyme','TsimmatGPCR','TsimmatIonChannel','TsimmatNuclearRecept']  # 原始数据集中靶标-靶标相似度\n",
    "\n",
    "\n",
    "# 2、mds 数据\n",
    "# 读取数据\n",
    "mds_data = sio.loadmat(path_mds+name_mds[curr_data_number]+'.mat')\n",
    "\n",
    "# drug 特征， drug 数 * 特征维数, 序号对应 interaction 列顺序\n",
    "drugAttribute = np.array(mds_data['drugAttribute'],dtype=float)\n",
    "# target 特征， target 数 * 特征维数, 序号对应 interaction 行顺序\n",
    "targetAttribute = np.array(mds_data['targetAttribute'],dtype=float)\n",
    "# 对 drug, target 特征进行归一化\n",
    "drug_clf = MinMaxScaler()\n",
    "drugAttribute = drug_clf.fit_transform(drugAttribute)\n",
    "target_clf = MinMaxScaler()\n",
    "targetAttribute = target_clf.fit_transform(targetAttribute)\n",
    "# ID\n",
    "drug_ID = [x[0] for x in mds_data['drugID'][:,0]]\n",
    "target_ID = [x[0] for x in mds_data['targetID'][:,0]]\n",
    "\n",
    "\n",
    "# 3、interaction 数据从原始数据 original 中读取\n",
    "# 读取数据\n",
    "interaction_data = sio.loadmat(path_original+name_interaction[curr_data_number]+'.mat')\n",
    "interaction_mat = np.array(interaction_data['matrix'], dtype=int)  # target * drug\n",
    "\n",
    "# ID benchmark\n",
    "drug_ID_benchmark = [x[0] for x in interaction_data['drugID'][:,0]]\n",
    "target_ID_benchmark = [x[0] for x in interaction_data['targetID'][:,0]]\n",
    "\n",
    "# 4、药物-药物、靶标-靶标相似度矩阵\n",
    "csim_data = sio.loadmat(path_original+name_csim[curr_data_number]+'.mat')\n",
    "csim_mat = np.array(csim_data['matrix'], dtype=float)  # drug * drug\n",
    "tsim_data = sio.loadmat(path_original+name_tsim[curr_data_number]+'.mat')\n",
    "tsim_mat = np.array(tsim_data['matrix'], dtype=float)  # target * target\n",
    "\n",
    "# 5、药物、靶标嵌入数据准备\n",
    "dgc = np.copy(interaction_mat)  # 互作矩阵\n",
    "dc = np.copy(csim_mat)  # 药物相似度矩阵\n",
    "dg = np.copy(tsim_mat)  # 靶标相似度矩阵\n",
    "\n",
    "dc = (dc + dc.T) / 2.0\n",
    "dg = (dg + dg.T) /2.0\n",
    "\n",
    "drug_label = np.array(drug_ID)\n",
    "target_label = np.array(target_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e7f45886-8679-4b31-aa9d-c5ec1360767a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss 24.3996\n",
      "Epoch 51 | Train Loss 0.0654\n",
      "target embedding...\n",
      "Epoch 1 | Train Loss 10.9080\n",
      "Epoch 51 | Train Loss 0.0501\n",
      "Epoch 101 | Train Loss 0.0334\n",
      "Epoch 151 | Train Loss 0.0350\n"
     ]
    }
   ],
   "source": [
    "param = {'drug_max_iters':drug_max_iters[curr_data_number],\n",
    "          'target_max_iters':target_max_iters[curr_data_number],\n",
    "         'dgc':dgc,\n",
    "         'dc':dc,\n",
    "         'drugAttribute':drugAttribute,\n",
    "         'dg':dg,\n",
    "         'targetAttribute':targetAttribute,\n",
    "         'drug_hidden_size':drug_hidden_sizes[curr_data_number],\n",
    "         'drug_out_size':drug_out_size[curr_data_number], \n",
    "         'drug_num_heads': [drug_num_heads_array[curr_data_number]],\n",
    "         'drug_dropout':drug_dropout,\n",
    "         'drug_label':drug_label,\n",
    "         'target_hidden_size':target_hidden_sizes[curr_data_number],\n",
    "         'target_out_size':target_out_size[curr_data_number],\n",
    "         'target_num_heads':[target_num_heads_array[curr_data_number]],\n",
    "         'target_dropout':target_dropout,\n",
    "         'target_label':target_label,\n",
    "         'path_to_csv': 'result/'+name_embedding[curr_data_number]+'.csv'}\n",
    "DTI_embedding(param=param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0c1d7bbb-0c7a-44fb-845d-23e063deef2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集：NuclearRecept\n",
      "lambda = 0.00, AUPR = 0.5132 +/- 0.0148, AUC = 0.9021 +/- 0.0025\n",
      "lambda = 0.01, AUPR = 0.5505 +/- 0.0151, AUC = 0.9582 +/- 0.0037\n",
      "lambda = 0.02, AUPR = 0.4611 +/- 0.0514, AUC = 0.9538 +/- 0.0028\n",
      "lambda = 0.03, AUPR = 0.6839 +/- 0.0431, AUC = 0.9495 +/- 0.0027\n",
      "lambda = 0.04, AUPR = 0.5467 +/- 0.0641, AUC = 0.9567 +/- 0.0008\n",
      "lambda = 0.05, AUPR = 0.5393 +/- 0.0174, AUC = 0.9470 +/- 0.0011\n",
      "lambda = 0.06, AUPR = 0.5356 +/- 0.0781, AUC = 0.9469 +/- 0.0194\n",
      "lambda = 0.07, AUPR = 0.6179 +/- 0.0535, AUC = 0.9612 +/- 0.0052\n",
      "lambda = 0.08, AUPR = 0.5296 +/- 0.0285, AUC = 0.9543 +/- 0.0023\n",
      "lambda = 0.10, AUPR = 0.5992 +/- 0.0187, AUC = 0.9465 +/- 0.0092\n",
      "lambda = 0.11, AUPR = 0.4792 +/- 0.0381, AUC = 0.9257 +/- 0.0065\n",
      "lambda = 0.12, AUPR = 0.5859 +/- 0.0401, AUC = 0.9494 +/- 0.0038\n",
      "lambda = 0.13, AUPR = 0.3997 +/- 0.0105, AUC = 0.9156 +/- 0.0054\n",
      "lambda = 0.14, AUPR = 0.4488 +/- 0.0321, AUC = 0.9550 +/- 0.0019\n",
      "lambda = 0.15, AUPR = 0.5958 +/- 0.0212, AUC = 0.9706 +/- 0.0033\n",
      "lambda = 0.16, AUPR = 0.4546 +/- 0.0296, AUC = 0.9164 +/- 0.0032\n",
      "lambda = 0.17, AUPR = 0.5584 +/- 0.0230, AUC = 0.9435 +/- 0.0026\n",
      "lambda = 0.18, AUPR = 0.3654 +/- 0.0189, AUC = 0.9236 +/- 0.0037\n",
      "lambda = 0.19, AUPR = 0.5146 +/- 0.0537, AUC = 0.9610 +/- 0.0006\n",
      "lambda = 0.20, AUPR = 0.4343 +/- 0.0315, AUC = 0.9498 +/- 0.0025\n"
     ]
    }
   ],
   "source": [
    "# FPRM 模型准备数据\n",
    "\n",
    "# 读取 embedding 数据\n",
    "embedding_data = pd.read_csv(path_embeding+name_embedding[curr_data_number]+'.csv', index_col=0, header=0)\n",
    "# embedding_data.head(-5)\n",
    "drug_embedding_df = embedding_data[embedding_data.index.str.contains('D')]\n",
    "target_embedding_df = embedding_data[embedding_data.index.str.contains('hsa')]\n",
    "# embedding_data.index.str.contains('hsa')  #  embedding_data.index 中包含 'hsa' \n",
    "# 对 drug, target 特征进行归一化\n",
    "drug_clf = MinMaxScaler()\n",
    "target_clf = MinMaxScaler()\n",
    "# data to DataFrame, index 为 ID\n",
    "drug_embedding_df = pd.DataFrame(data = drug_clf.fit_transform(drug_embedding_df.values), index=drug_embedding_df.index, columns=drug_embedding_df.columns)\n",
    "target_embedding_df = pd.DataFrame(data = target_clf.fit_transform(target_embedding_df.values), index=target_embedding_df.index, columns=target_embedding_df.columns)\n",
    "\n",
    "# 数据对齐\n",
    "# data to DataFrame, index 为 ID\n",
    "# 注意截取数据维度，与embedding维度保持一致\n",
    "drug_mds_df = pd.DataFrame(data=drugAttribute[:,0:drug_f_n], index=drug_ID)\n",
    "target_mds_df = pd.DataFrame(data=targetAttribute[:, 0:target_f_n], index=target_ID)\n",
    "# mds, embedding 数据以 interaction 的 ID benchmark 为基准对齐，\n",
    "# drug\n",
    "temp_df = pd.DataFrame(data=drug_ID_benchmark, index=drug_ID_benchmark, columns=['ID'])\n",
    "# mds 数据对齐\n",
    "drug_mds_df = pd.merge(temp_df, drug_mds_df, how='left', left_index=True, right_index=True, sort=False)\n",
    "drug_mds_df.drop(['ID'], axis=1, inplace=True)\n",
    "# embedding 数据对齐\n",
    "drug_embedding_df = pd.merge(temp_df, drug_embedding_df, how='left', left_index=True, right_index=True, sort=False)\n",
    "drug_embedding_df.drop(['ID'], axis=1, inplace=True)\n",
    "# target\n",
    "temp_df = pd.DataFrame(data=target_ID_benchmark, index=target_ID_benchmark, columns=['ID'])\n",
    "# mds 数据对齐\n",
    "target_mds_df = pd.merge(temp_df, target_mds_df, how='left', left_index=True, right_index=True, sort=False)\n",
    "target_mds_df.drop(['ID'], axis=1, inplace=True)\n",
    "# embedding 数据对齐\n",
    "target_embedding_df = pd.merge(temp_df, target_embedding_df, how='left', left_index=True, right_index=True, sort=False)\n",
    "target_embedding_df.drop(['ID'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# 特征加权\n",
    "# feature = lambda_feature * mds_feature + (1-lambda_feature) * embedding_feature\n",
    "# drug\n",
    "lambda_feature = lambda_features[curr_data_number]\n",
    "# lambda_feature = 0.01\n",
    "temp = (lambda_feature * drug_mds_df.values + (1.0-lambda_feature) * drug_embedding_df.values) / 2.0\n",
    "drug_feature_df = pd.DataFrame(data=temp, index=drug_mds_df.index, columns=drug_mds_df.columns)\n",
    "# target\n",
    "temp = (lambda_feature * target_mds_df.values + (1.0-lambda_feature) * target_embedding_df.values) / 2.0\n",
    "target_feature_df = pd.DataFrame(data=temp, index=target_mds_df.index, columns=target_mds_df.columns)\n",
    "\n",
    "\n",
    "# DTI 预测验证\n",
    "# 特别注意，由于整个模型存在局部最优，随机划分等因素，所以可以多重复运行几次，选择最优结果\n",
    "\n",
    "print('数据集：{}'.format(name_mds[curr_data_number]))\n",
    "\n",
    "# 正则化参数\n",
    "lambdas = np.linspace(0.001, 0.20, 20)\n",
    "# re_lambda = 1.0\n",
    "\n",
    "# 重复次数，交叉验证次数\n",
    "numSplit = 3\n",
    "folds = 10\n",
    "max_AUPR=0\n",
    "max_AUC=0\n",
    "for re_lambda in lambdas:\n",
    "    # 交叉验证\n",
    "    # interactionMatrix 需要形式  target * drug\n",
    "    res = CV_AUPR_AUC(interactionMatrix=interaction_mat,\n",
    "                      drugAttribute=drug_feature_df.values,\n",
    "                      targetAttribute=target_feature_df.values,\n",
    "                      re_lambda=re_lambda,\n",
    "                      numSplit=numSplit,\n",
    "                      kfolds=folds)\n",
    "    \n",
    "    if res[2]+res[3]>max_AUC:\n",
    "        max_AUC=res[2]+res[3]\n",
    "        optim_clf=res[4]\n",
    "   \n",
    "    print('lambda = {:.2f}, AUPR = {:.4f} +/- {:.4f}, AUC = {:.4f} +/- {:.4f}'.format(re_lambda, res[0], res[1], res[2], res[3]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3051b3f-b472-4b56-8833-20a10aadacca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
